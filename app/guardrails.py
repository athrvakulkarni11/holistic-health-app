"""
Guardrails & Protective Measures â€” Centralized safety module for the Healthcare AI Platform.

Provides:
  1. Input sanitization (XSS, injection prevention)
  2. PII detection & redaction (names, SSNs, emails, phones, addresses)
  3. Off-topic / adversarial prompt filtering
  4. Emergency & crisis detection (expanded keyword bank)
  5. Response validation & safety wrapping
  6. Rate limiting per session
  7. Content length enforcement
  8. File content safety scanning
"""

import re
import time
import hashlib
from datetime import datetime
from typing import Optional


# â”€â”€â”€ CONSTANTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MAX_MESSAGE_LENGTH = 5000
MAX_FILE_TEXT_LENGTH = 50000
MAX_MESSAGES_PER_MINUTE = 20
MAX_SESSIONS_PER_IP = 50

# Medical disclaimer appended to every AI analysis
MEDICAL_DISCLAIMER = (
    "âš•ï¸ **Medical Disclaimer:** This analysis is generated by an AI system and is "
    "intended for **informational and educational purposes only**. It does NOT constitute "
    "medical advice, diagnosis, or treatment. Always consult a qualified healthcare "
    "professional before making any medical decisions. If you are experiencing a medical "
    "emergency, call your local emergency number (e.g., 911) immediately."
)

# Concise version for chat
CHAT_DISCLAIMER = (
    "ðŸ”¹ *Reminder: I am an AI assistant, not a doctor. This is not medical advice. "
    "Always consult a healthcare professional for medical decisions.*"
)


# â”€â”€â”€ 1. INPUT SANITIZATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sanitize_input(text: str) -> str:
    """
    Sanitize user input to prevent injection attacks and clean malicious content.
    Strips HTML tags, script injections, and normalizes whitespace.
    """
    if not text:
        return ""

    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)

    # Remove potential script injections
    text = re.sub(r'javascript:', '', text, flags=re.IGNORECASE)
    text = re.sub(r'on\w+\s*=', '', text, flags=re.IGNORECASE)
    text = re.sub(r'data:\s*text/html', '', text, flags=re.IGNORECASE)

    # Remove null bytes and control characters (except newlines/tabs)
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)

    # Normalize excessive whitespace
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)

    # Trim to max length
    if len(text) > MAX_MESSAGE_LENGTH:
        text = text[:MAX_MESSAGE_LENGTH]

    return text.strip()


def sanitize_file_text(text: str) -> str:
    """Sanitize extracted file text with a larger length limit."""
    if not text:
        return ""

    # Remove HTML/script content
    text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r'<[^>]+>', '', text)

    # Remove null bytes
    text = re.sub(r'\x00', '', text)

    # Trim to file text limit
    if len(text) > MAX_FILE_TEXT_LENGTH:
        text = text[:MAX_FILE_TEXT_LENGTH]

    return text.strip()


# â”€â”€â”€ 2. PII DETECTION & REDACTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Patterns for detecting PII
PII_PATTERNS = {
    "ssn": re.compile(r'\b\d{3}[-.\s]?\d{2}[-.\s]?\d{4}\b'),
    "email": re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
    "phone": re.compile(r'\b(?:\+?1[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b'),
    "credit_card": re.compile(r'\b(?:\d{4}[-.\s]?){3}\d{4}\b'),
    "ip_address": re.compile(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b'),
    "date_of_birth": re.compile(
        r'\b(?:DOB|Date of Birth|Born|Birthday)\s*[:=]?\s*\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}\b',
        re.IGNORECASE
    ),
    "address": re.compile(
        r'\b\d{1,5}\s+(?:[A-Za-z]+\s){1,4}(?:Street|St|Avenue|Ave|Boulevard|Blvd|Drive|Dr|Road|Rd|Lane|Ln|Way|Court|Ct)\b',
        re.IGNORECASE
    ),
}

# Labels used in redacted output
PII_LABELS = {
    "ssn": "[SSN REDACTED]",
    "email": "[EMAIL REDACTED]",
    "phone": "[PHONE REDACTED]",
    "credit_card": "[CARD REDACTED]",
    "ip_address": "[IP REDACTED]",
    "date_of_birth": "[DOB REDACTED]",
    "address": "[ADDRESS REDACTED]",
}


def detect_pii(text: str) -> dict:
    """
    Detect PII in text. Returns a dict mapping PII type -> list of matches found.
    """
    findings = {}
    for pii_type, pattern in PII_PATTERNS.items():
        matches = pattern.findall(text)
        if matches:
            findings[pii_type] = matches
    return findings


def redact_pii(text: str) -> tuple[str, dict]:
    """
    Redact PII from text. Returns (redacted_text, findings).
    Only redacts in the text sent to the LLM; the user sees original.
    """
    findings = detect_pii(text)
    redacted = text

    for pii_type, matches in findings.items():
        label = PII_LABELS.get(pii_type, "[REDACTED]")
        for match in matches:
            redacted = redacted.replace(match, label)

    return redacted, findings


def has_sensitive_pii(text: str) -> bool:
    """Check if text contains high-risk PII that should trigger a warning."""
    high_risk = ["ssn", "credit_card"]
    findings = detect_pii(text)
    return any(pii_type in findings for pii_type in high_risk)


# â”€â”€â”€ 3. OFF-TOPIC / ADVERSARIAL FILTERING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Adversarial patterns â€” only clear prompt injection / harmful content
# Kept intentionally narrow to avoid blocking legitimate queries
ADVERSARIAL_PATTERNS = [
    # Prompt injection attempts (clear, intentional manipulation)
    r'ignore\s+(all\s+)?(previous|prior|above)\s+(instructions?|prompts?|rules?)',
    r'(?:forget|disregard|override)\s+(?:your|all|the)\s+(?:instructions?|guidelines?|rules?)',
    r'jailbreak',
    r'DAN\s+mode',

    # Harmful content (non-health related dangerous requests)
    r'how\s+to\s+(?:make|create|build|synthesize)\s+(?:a\s+)?(?:bomb|weapon|poison|explosive)',
    r'how\s+to\s+(?:hack|exploit|attack)\s+(?:a\s+)?(?:system|server|network|computer)',
    r'(?:illicit|illegal)\s+(?:drugs?|substances?)\s+(?:recipe|synthesis|manufacture)',
]

ADVERSARIAL_COMPILED = [re.compile(p, re.IGNORECASE) for p in ADVERSARIAL_PATTERNS]

# Off-topic subjects â€” only topics clearly unrelated to health/wellness
# NOTE: Relationship stress, mental health, politics-and-health are all ALLOWED
OFF_TOPIC_PATTERNS = [
    r'\b(?:stock|crypto|bitcoin|ethereum|trading|forex|investment)\s+(?:price|prediction|advice|tip)\b',
    r'\b(?:write|generate|create)\s+(?:a\s+)?(?:story|poem|essay|code|script|song)\b',
    r'\b(?:gambling|casino|poker|slot)\s+(?:strategy|tips?|tricks?|advice)\b',
]

OFF_TOPIC_COMPILED = [re.compile(p, re.IGNORECASE) for p in OFF_TOPIC_PATTERNS]


def check_adversarial(message: str) -> Optional[str]:
    """
    Check if the message contains adversarial/injection attempts.
    Returns a warning message if detected, None otherwise.
    Only triggers on clear, intentional manipulation â€” not ambiguous phrasing.
    """
    for pattern in ADVERSARIAL_COMPILED:
        if pattern.search(message):
            return (
                "âš ï¸ **Safety Notice**\n\n"
                "I detected content in your message that falls outside my capabilities "
                "as a health assistant. I'm designed to help with health, wellness, "
                "nutrition, and biomarker-related questions only.\n\n"
                "Please rephrase your question in the context of health and wellness, "
                "and I'll be happy to help."
            )
    return None


def check_off_topic(message: str) -> Optional[str]:
    """
    Check if the message is clearly off-topic for a health application.
    Returns a gentle redirect message if off-topic, None otherwise.
    
    NOTE: We intentionally keep this very narrow. Many topics that seem
    off-topic (relationships, stress, politics) have genuine health dimensions.
    The LLM's system prompt is trusted to stay on-topic for borderline cases.
    """
    for pattern in OFF_TOPIC_COMPILED:
        if pattern.search(message):
            return (
                "ðŸ¥ **Health Focus Reminder**\n\n"
                "I'm HolisticAI, specialized in **health, wellness, nutrition, and biomarker analysis**. "
                "The topic you asked about is outside my area of expertise.\n\n"
                "I can help you with:\n"
                "- ðŸ”¬ Understanding lab results and biomarkers\n"
                "- ðŸ¥— Nutrition and dietary guidance\n"
                "- ðŸ’Š Supplement and medication information\n"
                "- ðŸƒ Lifestyle and wellness recommendations\n"
                "- ðŸ§¬ Health condition information\n\n"
                "Please ask a health-related question and I'll do my best to help!"
            )
    return None


# â”€â”€â”€ 4. EMERGENCY & CRISIS DETECTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# DESIGN NOTE: We use multi-word phrases that indicate an ACTIVE emergency,
# not single words that could appear in informational queries.
# e.g. "having a heart attack right now" triggers, but
#      "what causes chest pain?" or "tell me about stroke" do NOT.
#
# Phrases that suggest the user is ASKING ABOUT a topic (educational)
# are handled normally by the LLM â€” only phrases suggesting an
# ACTIVE crisis bypass the LLM.

EMERGENCY_CATEGORIES = {
    "mental_health_crisis": {
        "keywords": [
            # These are specific enough that they almost always indicate crisis
            "kill myself", "want to die", "end my life",
            "end it all", "no reason to live", "better off dead",
            "harming myself", "cutting myself", "overdose on purpose",
            "planning to end", "goodbye forever", "final goodbye",
            "going to kill", "want to hurt myself",
        ],
        "response": (
            "**ðŸš¨ MENTAL HEALTH CRISIS â€” IMMEDIATE HELP AVAILABLE**\n\n"
            "I'm concerned about what you've shared. **You are not alone**, and help is available right now.\n\n"
            "**ðŸ“ž Immediate Resources:**\n"
            "- **988 Suicide & Crisis Lifeline:** Call or text **988** (US)\n"
            "- **Crisis Text Line:** Text **HELLO** to **741741**\n"
            "- **International Association for Suicide Prevention:** https://www.iasp.info/resources/Crisis_Centres/\n"
            "- **Emergency Services:** Call **911** (US) or your local emergency number\n\n"
            "**I am an AI and cannot provide emergency mental health support.** "
            "Please reach out to one of the resources above â€” trained professionals are available 24/7.\n\n"
            "ðŸ’™ Your life matters. Please talk to someone who can help."
        ),
    },
    "cardiac_emergency": {
        # Multi-word phrases indicating an active cardiac event
        # "chest pain" alone is intentionally excluded â€” users may ask about causes
        "keywords": [
            "having a heart attack", "i think i'm having a heart attack",
            "crushing chest pain", "severe chest pain right now",
            "pain radiating to left arm", "jaw pain and chest pain",
            "can't breathe and chest", "heart attack right now",
        ],
        "response": (
            "**ðŸš¨ POTENTIAL CARDIAC EMERGENCY**\n\n"
            "The symptoms you're describing could indicate a **serious cardiac event**.\n\n"
            "**âš¡ TAKE IMMEDIATE ACTION:**\n"
            "1. **Call 911** (or your local emergency number) **immediately**\n"
            "2. **Chew an aspirin** (325mg) if not allergic and available\n"
            "3. **Sit upright** and try to remain calm\n"
            "4. **Do NOT drive yourself** to the hospital\n"
            "5. **Unlock your door** so paramedics can enter\n\n"
            "**â° Time is critical.** Every minute matters for cardiac events.\n\n"
            "**I am an AI and cannot provide emergency medical care.** "
            "Please call emergency services RIGHT NOW."
        ),
    },
    "stroke_emergency": {
        # "stroke" alone excluded â€” users ask "what is a stroke?" legitimately
        "keywords": [
            "having a stroke", "i think i'm having a stroke",
            "face drooping right now", "sudden numbness in face",
            "can't move my arm suddenly", "speech slurred suddenly",
            "worst headache of my life",
        ],
        "response": (
            "**ðŸš¨ POTENTIAL STROKE â€” ACT F.A.S.T.**\n\n"
            "The symptoms you describe may indicate a **stroke**. Use the **F.A.S.T.** method:\n\n"
            "- **F**ace: Ask the person to smile. Does one side droop?\n"
            "- **A**rms: Ask them to raise both arms. Does one drift downward?\n"
            "- **S**peech: Ask them to repeat a simple phrase. Is their speech slurred?\n"
            "- **T**ime: If ANY of these are present, **call 911 immediately**\n\n"
            "**â° Stroke treatment is most effective within the first 3 hours.**\n\n"
            "**I am an AI and cannot provide emergency medical care.** "
            "Call emergency services RIGHT NOW."
        ),
    },
    "severe_allergic_reaction": {
        "keywords": [
            "anaphylaxis right now", "anaphylactic shock",
            "throat closing up", "throat is swelling",
            "can't swallow or breathe", "tongue swelling up",
            "severe allergic reaction right now",
        ],
        "response": (
            "**ðŸš¨ SEVERE ALLERGIC REACTION (ANAPHYLAXIS)**\n\n"
            "This could be a **life-threatening allergic reaction**.\n\n"
            "**âš¡ TAKE IMMEDIATE ACTION:**\n"
            "1. **Use an epinephrine auto-injector (EpiPen)** if available\n"
            "2. **Call 911** immediately\n"
            "3. **Lie down** with legs elevated (unless having trouble breathing)\n"
            "4. **Do NOT take oral medications** if throat is swelling\n"
            "5. **A second reaction can occur** â€” get to an ER even if symptoms improve\n\n"
            "**I am an AI and cannot provide emergency medical care.** "
            "Please call emergency services immediately."
        ),
    },
    "general_emergency": {
        # Only phrases indicating an active, ongoing emergency
        "keywords": [
            "call 911", "bleeding profusely right now", "heavy bleeding won't stop",
            "someone is unconscious", "person not breathing",
            "having a seizure right now", "i've been poisoned",
            "someone collapsed",
        ],
        "response": (
            "**ðŸš¨ MEDICAL EMERGENCY WARNING**\n\n"
            "It sounds like you or someone nearby may be experiencing a **medical emergency**.\n\n"
            "**âš¡ TAKE IMMEDIATE ACTION:**\n"
            "1. **Call 911** or your local emergency number **immediately**\n"
            "2. Go to the **nearest emergency room**\n"
            "3. If someone is unconscious, check for breathing and begin CPR if trained\n\n"
            "**I am an AI and am NOT equipped to handle medical emergencies.** "
            "Professional help is critical â€” please act now."
        ),
    },
}


def check_emergency(message: str) -> Optional[tuple[str, str]]:
    """
    Check if the message indicates an ACTIVE emergency/crisis.
    Returns (category, response_message) if emergency detected, None otherwise.
    
    Intentionally uses multi-word phrases to avoid false positives on
    informational queries like "what causes chest pain?" or "tell me about stroke".
    """
    message_lower = message.lower()

    for category, data in EMERGENCY_CATEGORIES.items():
        for keyword in data["keywords"]:
            if keyword in message_lower:
                return (category, data["response"])

    return None


# â”€â”€â”€ 5. RESPONSE VALIDATION & SAFETY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Truly dangerous patterns â€” things that should NEVER appear in health AI output
# NOTE: Medication suggestions ARE allowed. We validate them separately.
DANGEROUS_RESPONSE_PATTERNS = [
    r'(?:take|consume|ingest)\s+(?:the\s+)?(?:all|entire|whole)\s+(?:bottle|package|box)',
    r'(?:lethal|fatal|deadly)\s+(?:dose|dosage|amount)',
    r'(?:mix|combine)\s+(?:with\s+)?(?:bleach|ammonia)',
    r'(?:guaranteed|100%|definitely)\s+(?:cure|fix|heal|treat)',
    r'(?:replace|substitute)\s+(?:your\s+)?(?:prescribed\s+)?(?:medication|prescription)\s+with\s+(?:this|these|herbal|natural)',
]

DANGEROUS_RESPONSE_COMPILED = [re.compile(p, re.IGNORECASE) for p in DANGEROUS_RESPONSE_PATTERNS]


def validate_response(response_text: str) -> str:
    """
    Validate AI response for dangerous content and add safety measures.
    Returns the (potentially modified) response.
    """
    if not response_text:
        return response_text

    # Check for dangerous content patterns
    for pattern in DANGEROUS_RESPONSE_COMPILED:
        if pattern.search(response_text):
            response_text = (
                "âš ï¸ *Some content in this response may require professional verification.* "
                "Always consult your healthcare provider before making changes to your "
                "medications or treatment plan.\n\n" + response_text
            )
            break  # Only add warning once

    # Post-validate any medication mentions
    response_text = validate_medication_mentions(response_text)

    return response_text


# â”€â”€â”€ 5b. MEDICATION POST-VALIDATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# The AI is allowed to suggest commonly used medications, but we
# post-validate mentions against a known list. Unknown drugs get
# flagged with a verification note.

KNOWN_MEDICATIONS = {
    # Common OTC
    "acetaminophen", "tylenol", "ibuprofen", "advil", "motrin", "aspirin",
    "naproxen", "aleve", "cetirizine", "zyrtec", "loratadine", "claritin",
    "diphenhydramine", "benadryl", "famotidine", "pepcid", "omeprazole",
    "prilosec", "loperamide", "imodium", "guaifenesin", "mucinex",
    "dextromethorphan",

    # Common Rx (widely prescribed, well-known)
    "metformin", "glucophage", "lisinopril", "amlodipine", "atorvastatin",
    "lipitor", "rosuvastatin", "crestor", "simvastatin", "zocor",
    "levothyroxine", "synthroid", "metoprolol", "losartan",
    "hydrochlorothiazide", "amoxicillin", "azithromycin", "zithromax",
    "ciprofloxacin", "prednisone", "prednisolone", "montelukast",
    "singulair", "albuterol", "fluticasone", "pantoprazole",
    "esomeprazole", "nexium", "sertraline", "zoloft", "escitalopram",
    "lexapro", "fluoxetine", "prozac", "duloxetine", "cymbalta",
    "gabapentin", "pregabalin", "lyrica", "tramadol",
    "insulin", "warfarin", "coumadin", "apixaban", "eliquis",
    "clopidogrel", "plavix", "empagliflozin", "jardiance",
    "sitagliptin", "januvia", "glimepiride", "pioglitazone",

    # Supplements (commonly suggested in health contexts)
    "vitamin d", "vitamin d3", "cholecalciferol", "vitamin b12",
    "cyanocobalamin", "methylcobalamin", "folic acid", "folate",
    "iron", "ferrous sulfate", "ferrous fumarate", "calcium",
    "magnesium", "zinc", "omega-3", "fish oil", "probiotics",
    "melatonin", "vitamin c", "ascorbic acid", "biotin",
    "coenzyme q10", "coq10", "turmeric", "curcumin",
}

# Pattern to find medication-like mentions in text
MEDICATION_MENTION_PATTERN = re.compile(
    r'(?:take|prescribe|suggest|recommend|consider|try|use|start)'
    r'(?:s|d|ing|ed)?\s+'
    r'([A-Za-z][A-Za-z0-9\-\s]{2,30}?)'
    r'(?:\s*\(|\s*[,.]|\s+\d|\s+for\b|\s+to\b|\s+if\b|\s+as\b)',
    re.IGNORECASE
)


def validate_medication_mentions(response_text: str) -> str:
    """
    Post-validate medication mentions in an AI response.
    
    - Known medications: allowed through without changes
    - Unknown drug-like mentions: flagged with a verification note
    
    This allows the AI to suggest helpful medications while ensuring
    the user is prompted to verify anything unfamiliar.
    """
    matches = MEDICATION_MENTION_PATTERN.findall(response_text)
    if not matches:
        return response_text

    unknown_drugs = []
    for match in matches:
        drug_name = match.strip().lower()
        # Skip very short matches or common non-drug words
        if len(drug_name) < 3:
            continue
        skip_words = {
            "the", "this", "that", "these", "your", "their", "some",
            "any", "more", "less", "each", "with", "them", "into",
            "food", "foods", "diet", "water", "exercise", "rest",
            "sleep", "walking", "stress", "breathing",
        }
        if drug_name in skip_words:
            continue
        # Check against known list
        if drug_name not in KNOWN_MEDICATIONS:
            # Check if any known medication is a substring (handles "vitamin d3 supplement")
            if not any(known in drug_name or drug_name in known for known in KNOWN_MEDICATIONS):
                unknown_drugs.append(match.strip())

    if unknown_drugs:
        unique_unknown = list(set(unknown_drugs))[:3]  # Cap at 3 to avoid noise
        drug_list = ", ".join(f"**{d}**" for d in unique_unknown)
        response_text += (
            f"\n\n---\n"
            f"ðŸ’Š *Medication note: This response mentions {drug_list}. "
            f"Please verify these with your doctor or pharmacist before use, "
            f"as I may not have the latest safety information for all medications.*"
        )

    return response_text


# â”€â”€â”€ 6. RATE LIMITING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class RateLimiter:
    """Simple in-memory rate limiter for chat sessions."""

    def __init__(self, max_requests: int = MAX_MESSAGES_PER_MINUTE, window_seconds: int = 60):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self._requests: dict[str, list[float]] = {}

    def is_allowed(self, identifier: str) -> bool:
        """Check if the identifier (session_id / IP) is within rate limits."""
        now = time.time()
        cutoff = now - self.window_seconds

        if identifier not in self._requests:
            self._requests[identifier] = []

        # Clean old entries
        self._requests[identifier] = [
            t for t in self._requests[identifier] if t > cutoff
        ]

        if len(self._requests[identifier]) >= self.max_requests:
            return False

        self._requests[identifier].append(now)
        return True

    def get_wait_time(self, identifier: str) -> float:
        """Get seconds until the next request is allowed."""
        if identifier not in self._requests or not self._requests[identifier]:
            return 0.0

        now = time.time()
        cutoff = now - self.window_seconds
        valid = [t for t in self._requests[identifier] if t > cutoff]

        if len(valid) < self.max_requests:
            return 0.0

        oldest = min(valid)
        return max(0.0, oldest + self.window_seconds - now)

    def cleanup(self):
        """Remove expired entries to prevent memory growth."""
        now = time.time()
        cutoff = now - self.window_seconds
        expired_keys = []

        for key, timestamps in self._requests.items():
            valid = [t for t in timestamps if t > cutoff]
            if valid:
                self._requests[key] = valid
            else:
                expired_keys.append(key)

        for key in expired_keys:
            del self._requests[key]


# Global rate limiter instance
rate_limiter = RateLimiter()


# â”€â”€â”€ 7. FILE CONTENT SAFETY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scan_file_content(content: bytes, filename: str) -> tuple[bool, str]:
    """
    Scan file content for potentially malicious payloads.
    Returns (is_safe, reason).
    """
    # Check for executable signatures
    dangerous_signatures = [
        b'MZ',           # Windows executable
        b'\x7fELF',      # Linux executable
        b'#!/',          # Shell script
        b'<?php',        # PHP script
        b'<%',           # ASP script
    ]

    for sig in dangerous_signatures:
        if content[:len(sig)] == sig:
            return False, f"File appears to be an executable or script, not a document."

    # Check for embedded macros in Office documents (simplified check)
    if filename.lower().endswith('.docx'):
        if b'vbaProject' in content or b'VBAProject' in content:
            return False, "File contains macros which are not allowed for security reasons."

    # Check for extremely large files that might be decompression bombs
    if len(content) > 50 * 1024 * 1024:  # 50 MB
        return False, "File is too large for processing."

    return True, "File passed safety scan."


# â”€â”€â”€ 8. CRITICAL BIOMARKER VALUES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CRITICAL_VALUES = {
    "hemoglobin": {"critical_low": 7.0, "critical_high": 20.0, "unit": "g/dL"},
    "fasting_glucose": {"critical_low": 50.0, "critical_high": 400.0, "unit": "mg/dL"},
    "hba1c": {"critical_low": None, "critical_high": 12.0, "unit": "%"},
    "sgpt_alt": {"critical_low": None, "critical_high": 1000.0, "unit": "U/L"},
    "tsh": {"critical_low": 0.1, "critical_high": 50.0, "unit": "mIU/L"},
    "triglycerides": {"critical_low": None, "critical_high": 500.0, "unit": "mg/dL"},
    "ldl": {"critical_low": None, "critical_high": 300.0, "unit": "mg/dL"},
    "total_cholesterol": {"critical_low": None, "critical_high": 400.0, "unit": "mg/dL"},
    "ferritin": {"critical_low": 5.0, "critical_high": 1000.0, "unit": "ng/mL"},
    "hs_crp": {"critical_low": None, "critical_high": 10.0, "unit": "mg/L"},
}


def check_critical_values(biomarkers: dict) -> list[dict]:
    """
    Check biomarkers against critical/panic values.
    Returns list of critical findings that require urgent attention.
    """
    critical_findings = []

    for key, value in biomarkers.items():
        if value is None:
            continue

        critical = CRITICAL_VALUES.get(key)
        if not critical:
            continue

        try:
            val = float(value)
        except (ValueError, TypeError):
            continue

        if critical["critical_low"] is not None and val < critical["critical_low"]:
            critical_findings.append({
                "biomarker": key,
                "value": val,
                "unit": critical["unit"],
                "direction": "critically_low",
                "threshold": critical["critical_low"],
                "message": f"âš ï¸ CRITICAL: {key} is {val} {critical['unit']} â€” below critical threshold of {critical['critical_low']} {critical['unit']}. Seek immediate medical attention.",
            })

        if critical["critical_high"] is not None and val > critical["critical_high"]:
            critical_findings.append({
                "biomarker": key,
                "value": val,
                "unit": critical["unit"],
                "direction": "critically_high",
                "threshold": critical["critical_high"],
                "message": f"âš ï¸ CRITICAL: {key} is {val} {critical['unit']} â€” above critical threshold of {critical['critical_high']} {critical['unit']}. Seek immediate medical attention.",
            })

    return critical_findings


# â”€â”€â”€ 9. AUDIT LOGGING (LIGHT) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def log_safety_event(event_type: str, details: str, session_id: str = None):
    """
    Log safety-relevant events for auditing.
    In production, this should write to a proper logging system.
    """
    timestamp = datetime.now().isoformat()
    session_info = f" [session={session_id}]" if session_id else ""
    print(f"[SAFETY:{event_type}]{session_info} {timestamp} â€” {details}")


# â”€â”€â”€ COMPOSITE GUARDRAIL CHECK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_input_guardrails(
    message: str,
    session_id: str = None,
) -> tuple[str, Optional[str]]:
    """
    Run all input guardrails on a user message.

    Returns:
        (sanitized_message, intervention_response)

    If intervention_response is not None, the normal AI flow should be
    bypassed and the intervention returned directly to the user.
    """
    # 1. Sanitize
    clean_message = sanitize_input(message)

    if not clean_message:
        return "", "Please enter a valid message."

    # 2. Rate limiting
    identifier = session_id or "anonymous"
    if not rate_limiter.is_allowed(identifier):
        wait_time = rate_limiter.get_wait_time(identifier)
        log_safety_event("RATE_LIMIT", f"Rate limit hit for {identifier}", session_id)
        return clean_message, (
            f"ðŸ• **Rate Limit Reached**\n\n"
            f"You're sending messages too quickly. Please wait "
            f"**{int(wait_time)} seconds** before trying again. "
            f"This limit exists to ensure fair usage for all users."
        )

    # 3. Emergency check (highest priority)
    emergency_result = check_emergency(clean_message)
    if emergency_result:
        category, response = emergency_result
        log_safety_event("EMERGENCY", f"Category: {category}", session_id)
        return clean_message, response

    # 4. Adversarial / prompt injection check
    adversarial_result = check_adversarial(clean_message)
    if adversarial_result:
        log_safety_event("ADVERSARIAL", "Adversarial input detected", session_id)
        return clean_message, adversarial_result

    # 5. Off-topic check
    off_topic_result = check_off_topic(clean_message)
    if off_topic_result:
        log_safety_event("OFF_TOPIC", "Off-topic message detected", session_id)
        return clean_message, off_topic_result

    # 6. PII warning (warn but don't block)
    if has_sensitive_pii(clean_message):
        log_safety_event("PII_WARNING", "Sensitive PII detected in input", session_id)
        # We don't block â€” but we redact before sending to LLM

    # All checks passed
    return clean_message, None


def run_output_guardrails(response_text: str) -> str:
    """
    Run all output guardrails on an AI response.
    Returns the validated and safety-wrapped response.
    
    NOTE: We intentionally do NOT force-append a disclaimer to every message.
    The system prompt instructs the AI to include appropriate disclaimers
    naturally. Force-appending makes responses feel robotic and repetitive.
    We only intervene if the response contains dangerous content patterns.
    """
    # Validate for dangerous content + post-validate medication mentions
    response_text = validate_response(response_text)

    return response_text
